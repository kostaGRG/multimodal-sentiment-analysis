{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extract predictions from vader tool, a lexicon approach for sentiment analysis"
      ],
      "metadata": {
        "id": "fNi3go3yg2nW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "TrQpI8PPhEhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect with Google Drive files"
      ],
      "metadata": {
        "id": "LSG5xvmihgcT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm4ky_Bn2nlJ",
        "outputId": "02f13ecf-fcbb-4402-cfca-ee2d70a376a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install/Import necessary libraries"
      ],
      "metadata": {
        "id": "yWVSNC_ohmKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment\n",
        "!pip install contractions\n",
        "!pip install sentencepiece\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import re #regular expressions\n",
        "import contractions\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A4KnxKU2s5O",
        "outputId": "d1cccc8e-4904-48d0-f828-bf3567b4b478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "qHJQpOo4hwRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read from an h5py file, the file's location is given with the \"path\" argument.\n",
        "def read_hdf5(path):\n",
        "    read_file = h5py.File(path, 'r')\n",
        "\n",
        "    feature_names = list(read_file.keys())\n",
        "    loaded_data = []\n",
        "\n",
        "    for name in feature_names:\n",
        "        dataset = read_file[name][:]\n",
        "        if dataset.dtype == np.dtype('object'):\n",
        "            dataset = np.array([x.decode('utf-8') for x in dataset])\n",
        "        loaded_data.append((name, dataset))\n",
        "\n",
        "    return loaded_data\n",
        "\n",
        "# Load MVSA dataset that we have stored when the cleaning was done.\n",
        "# Use the mode argument to select between the pair of texts/images (mode=1),\n",
        "# only texts (mode=2) and only images (mode=3).\n",
        "def load_mvsa_data(path,mode):\n",
        "    data = read_hdf5(path)\n",
        "    if mode == 1: #multimodal\n",
        "      for x in data:\n",
        "          if x[0] == 'texts':\n",
        "              texts = x[1]\n",
        "          if x[0] == 'multimodal-labels':\n",
        "              labels = x[1]\n",
        "          if x[0] == 'images':\n",
        "              images = x[1]\n",
        "      return texts, images, labels\n",
        "\n",
        "    elif mode == 2: # text only\n",
        "      for x in data:\n",
        "          if x[0] == 'texts':\n",
        "              texts = x[1]\n",
        "          if x[0] == 'text-labels':\n",
        "              text_labels = x[1]\n",
        "      return texts,text_labels\n",
        "\n",
        "    elif mode == 3: # image only\n",
        "      for x in data:\n",
        "          if x[0] == 'images':\n",
        "              images = x[1]\n",
        "          if x[0] == 'image-labels':\n",
        "              image_labels = x[1]\n",
        "      return images,image_labels\n",
        "\n",
        "# Apply selected preprocessing steps on the texts of the dataset\n",
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    # Replace '&amp;' with '&'\n",
        "    # Remove trailing whitespace\n",
        "    # Remove words that contain only digits\n",
        "    # Remove contractions, example: I'll --> I will\n",
        "    text = re.sub('RT '+r'(@.*?)[\\s]', '', text)\n",
        "    text = re.sub(r'(@.*?)[\\s]', '', text)\n",
        "    text = re.sub(r'#','',text)\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    #text = re.sub(r'\\b\\d+\\b','', text)\n",
        "    text = contractions.fix(text)\n",
        "    return text\n",
        "\n",
        "# Calculate sentiment scores using vader tool for sentiment analysis\n",
        "# The output has 4 values:\n",
        "# neg_values: array of possibilities of negative sentiment\n",
        "# neu_values: array of possibilites of neutral sentiment\n",
        "# pos_values: array of possibilities of positive sentiment\n",
        "# compound_values: array of a compound value that represents a direct prediction of the sentiment\n",
        "def sentiment_scores_from_vader(texts):\n",
        "  sentIntensityAnalyzer = SentimentIntensityAnalyzer()\n",
        "  neg_values = []\n",
        "  neu_values = []\n",
        "  pos_values = []\n",
        "  compound_values = []\n",
        "\n",
        "  for sentence in texts:\n",
        "      sentiment_dict = sentIntensityAnalyzer.polarity_scores(sentence)\n",
        "      neg_values.append(sentiment_dict['neg'])\n",
        "      neu_values.append(sentiment_dict['neu'])\n",
        "      pos_values.append(sentiment_dict['pos'])\n",
        "      compound_values.append(sentiment_dict['compound'])\n",
        "\n",
        "  return np.array([neg_values,neu_values,pos_values,compound_values]).T"
      ],
      "metadata": {
        "id": "ceYsXkaq20VG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate and save predictions of vader tool"
      ],
      "metadata": {
        "id": "XIb7paNHjSLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEXT_DATA_PATH = './drive/My Drive/sentiment-analysis/mvsa-multiple-19600_text.hdf5'\n",
        "TEXT_DATA_PATH = './drive/My Drive/sentiment-analysis/mvsa-single-4511_multimodal.hdf5'\n",
        "texts,_,_= load_mvsa_data(TEXT_DATA_PATH, 1)\n",
        "\n",
        "texts = [text_preprocessing(text) for text in texts]\n",
        "vaderValues = sentiment_scores_from_vader(texts)\n",
        "\n",
        "np.save(\"vader_values.npy\",vaderValues)"
      ],
      "metadata": {
        "id": "dN8GHR2X3XpS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}